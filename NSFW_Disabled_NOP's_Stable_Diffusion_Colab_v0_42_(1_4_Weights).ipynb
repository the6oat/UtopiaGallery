{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/the6oat/UtopiaGallery/blob/main/NSFW_Disabled_NOP's_Stable_Diffusion_Colab_v0_42_(1_4_Weights).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# NSFW Disabled: NOP & WAS's Stable Diffusion Colab v0.42 (1.4 Weights)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stablity.AI Model Terms of Use\n",
        "By using this Notebook, you agree to the following Terms of Use, and license\n",
        "\n",
        "This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.\n",
        "\n",
        "The CreativeML OpenRAIL License specifies:\n",
        "\n",
        "You can't use the model to deliberately produce nor share illegal or harmful outputs or content\n",
        "\n",
        "CompVis claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\n",
        "\n",
        "You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)\n",
        "\n",
        "Please read the full license here: https://huggingface.co/spaces/CompVis/stable-diffusion-license"
      ],
      "metadata": {
        "id": "fSanMIbKMydc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y6RXjS1tTji"
      },
      "source": [
        "#Info\n",
        "\n",
        "Newest version can be found at: https://colab.research.google.com/drive/1jUwJ0owjigpG-9m6AI_wEStwimisUE17\n",
        "\n",
        "WAS will be developing this colab with me, he has been doing great work! The logical thing would be to include him here, throw him some kudos for the great work done when you see him! :)\n",
        "\n",
        "Trying to make this a one-stop shop for various programs + a GOTO guide on how to install everything locally. If you have suggestions, bug reports, or implementations, feel free to contact me on Discord and/or leave a comment via colab's comment feature. NOP#1337\n",
        "\n",
        "\n",
        "Changelog:\n",
        "- v0.4: Inpainting now supported within the main diffusers cell. Additionally, if there is no mask image provided, you can draw your own within the colab (Mobile unfortunately not supported YET)\n",
        "- v0.41: Added a `VRAM_OVER_SPEED` implementation that is based on a discussion in the diffusers Github. Checking this option will prioritize VRAM usage over creation speed. That being said, haven't tested it extensively\n",
        "- v0.42: added waifu as a model thanks to Alamgir#1781. Use at your own risk, haven't tested it. If you want to use it, just select it under `MODEL_ID`. It is trained on danbooru data therefore if you want to generate an anime character with a japanese name its better to put the last name first ((Since this is 0.42 does that mean that Waifu is the meaning of life?)))\n",
        "\n",
        "![](https://cdn.discordapp.com/attachments/1004159122335354970/1015717263501963425/unknown.png)\n",
        "\n",
        "By NOP#1337 & WAS#0263\n",
        "\n",
        "Thank you ! Nee#9981, Alamgir#1781, and Queen#0613 for ~~breaking my stuff~~ bug hunting!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeIWggi6TGH0"
      },
      "source": [
        "#Scheduler/Sampler Study"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYJ8pdUoTilg"
      },
      "source": [
        "Kudos to scarletpenn#7121 !\n",
        "\n",
        "![](https://cdn.discordapp.com/attachments/1002602742667280404/1014634578226462740/K-LMS_vs_PNDM_vs_DDIM_0-1.0.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPyJJ2z-RJB7"
      },
      "source": [
        "#Changelog/Credits/FAQ/TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2uscKB2ROiU"
      },
      "source": [
        "Changelog:\n",
        "\n",
        "- v0.1: Colab creation\n",
        "- v0.11: Google Drive option for TXT2IMG and some error corrections\n",
        "- v0.12: Added more options to TXT2IMG\n",
        "- v0.13: Diffusers added a feature which broke the pipeline with the current implementation, reverted back to an older version\n",
        "- v0.14: Added in full precision in the diffuser method\n",
        "- v0.15: Added in file saving in drive for diffusers\n",
        "- v0.16: Added in prompt saving\n",
        "- v0.17: Added in the new weights and disabled the NSFW check\n",
        "- v0.18: Minor adjustments and more details saved in prompt saving\n",
        "- v0.19: Added in modifier experiments in Diffusers + example. More options to experiment will come with future updates\n",
        "- v0.20: Low VRAM patch is fixed. Getting 10 it/s with it on with a V100\n",
        "- v0.21: Diffusers now has an upscaler (Real-ESRGAN) <-- just updated to GFPGAN\n",
        "- v0.22: Added in a small little fun randomizer\n",
        "- v0.23: Now I support both upscalers. GFP is really good at faces but kind of sucks at upscaling. If you want the best of two worlds choose \"Both\" as an upscaler. T4 may have problems with one or both of them, looking at a fix for that (May get lucky with Real-ESRGAN).\n",
        "- v0.30: A complete code overhaul by WAS#0263 and a bunch of stuff added. With an overhaul, there could also different bugs. Shoot me ( NOP#1337 ) a discord message when you find one and tell WAS that he is awesome when you see him! If there are major bugs, I'll fix them as soon as I can\n",
        "- v0.31: Forgot to mention last update: No more huggingface login, that's all built-in now. Also, we have new facial enhancement. --> CodeFormer. It's like GFP but not as strong + with a nifty slider\n",
        "- v0.32: Added options for samplers (still having problems with other ones). Also added an option to sharpen the image.\n",
        "- v0.33: Added ddim & ETA for DDIM. Also trying to dim down some more VRAM\n",
        "- v0.34: Merged setup into the render cell.\n",
        "- v0.35: Just some backend stuff. Will behave differently, don't be alarmed. Sometimes VRAM got stuck and this should fix it. Also added a 'SKIP_PREVIEW' button to see if this can fix connectivity issues after running the colab.\n",
        "- v0.36: IMG2IMG (inits) are now available. If you like drawing, then turn on USE_INIT and empty out INIT_IMAGE\n",
        "- v0.37: Better drawing feature if no init image is provided\n",
        "- v0.38: Added options for weights and diffuser versions\n",
        "- v0.39: Added a (bad) first integration of inpainting. Plans are to have a full-on editor for masks similarly to img2img/init. You need to run diffusers first in order to set up the environment though. When you are done, make sure to clean up the vram in order to help prevent Out Of Memory errors\n",
        "\n",
        "Credits:\n",
        "- WAS#0263 for giving great advice, coding tips, code, and recommendations. A MASSIVE help overhauling this thing\n",
        "- ùìëùìµùì™ùì∑ùì¨ùììùìÆùìûùìØùìØùì≤ùì¨ùì≤ùì™ùìµ#2485 for inspiring me to put an upscaler in the colab and for bug hunting\n",
        "- Gecktendo#8043 for helping with the default prompt\n",
        "- Original TXT2IMG Notebook: Lucas Ferreira da Silva, Madams, Greg Turk\n",
        "\n",
        "FAQ:\n",
        "\n",
        "Q: What is the difference between Diffusers and TXT2IMG?\n",
        "\n",
        "A: Diffusers is the Huggingface Python Library and TXT2IMG is from the Stability AI Github. They both do the same thing, but differently. Whichever you want to use is just personal preference.\n",
        "\n",
        "Q: Which one should I use?\n",
        "\n",
        "A: Really just personal preference. For me: Right now I am heavily concentrating on diffusers just because it's a tad easier to work with.\n",
        "\n",
        "TODO:\n",
        "- Saving weights in Google Drive\n",
        "- Option to load a config file to load in preset settings\n",
        "- Support/Bugfixing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Donate"
      ],
      "metadata": {
        "id": "h-qNQtzBRbjo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Originally did not want to include this, but people keep on asking me if they could donate. So if you want to fuel my caffeeine addiction: https://www.buymeacoffee.com/NOP1337 . 100% of the donations will go towards coffee\n",
        "\n",
        "Don't feel obligated to, this will always remain free no matter what"
      ],
      "metadata": {
        "id": "PmIPmy4jRegy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5V9MWbFtpNi"
      },
      "source": [
        "#GPU Info\n",
        " (If it throws an error here, go to Runtime, then click \"Change Runtime Type\" and then select \"GPU\"). There's also a chance that Colab put you on a GPU timeout if this is set and it still throws an error"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qxlZ660jRzVg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_ekR-LW6trWG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8de9dfa1-d6a8-4edf-edad-1cdfd0941924"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHaZZ0uKti1c"
      },
      "source": [
        "# Diffusers Method\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUlgKvW_EfqX"
      },
      "source": [
        "Merged everything into one cell. Just click Render and you are good-to-go. First run will take a while since it has to setup everything, but afterwards it should be quick to render with every go-around (or at least until the session disconnects)\n",
        "\n",
        "If anyone having problems running this colab on mobile device then try checking the `Desktop Site` in chrome from the menu from top right corner . (Kudos to Rohan Singh)\n",
        "\n",
        "Went back to speedy mode, but will take longer for the program to actually stop when you hit stop (Can't find a good solution for this). When you spin it up again though it should be lightning fast. I THINK I have the stuck vram issue mitigated, but not sure\n",
        "\n",
        "Will also be looking at adding a mobile friendly drawing feature\n",
        "\n",
        "<b>If you have colab connectivity issues, keep the `NUM_ITERS` low!</b>\n",
        "\n",
        "\n",
        "Currently figuring out how to best include textual inversion into the notebook, in the meantime if you want to try it, here are the notebooks:\n",
        "\n",
        "Training: https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb\n",
        "\n",
        "Inference: https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ucr5_i21xSjv",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# Reorg Template\n",
        "\n",
        "class Manager:\n",
        "  def __init__(self):\n",
        "    # self.pipeline_assigner = {}\n",
        "    pass\n",
        "\n",
        "  class Diffusion:\n",
        "\n",
        "    def get_settings():\n",
        "      pass\n",
        "\n",
        "    def set_settings():\n",
        "      pass\n",
        "\n",
        "    def patch_nsfw():\n",
        "      import shutil\n",
        "      import os\n",
        "      os.remove('/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py')\n",
        "      if ENABLE_NSFW_FILTER:\n",
        "        shutil.copyfile(f'/content/safety_checker.py', '/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py')\n",
        "      else:\n",
        "        shutil.copyfile(f'/content/safety_checker_patched.py', '/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py')\n",
        "\n",
        "    def make_pipe():\n",
        "      pass\n",
        "\n",
        "    def cache_pipe():\n",
        "      pass\n",
        "\n",
        "    def clear_pipe():\n",
        "      pass\n",
        "\n",
        "    class Upscaler:\n",
        "      pass\n",
        "\n",
        "    class Scheduler:\n",
        "      pass\n",
        "\n",
        "    class Methods:\n",
        "      class Prompt:\n",
        "        pass\n",
        "\n",
        "      class ImgToImg:\n",
        "        pass\n",
        "\n",
        "      class Inpaintin:\n",
        "        pass\n",
        "\n",
        "\n",
        "class Colab:\n",
        "  def clean_env():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  class Images:\n",
        "    def resize_image():\n",
        "      pass\n",
        "\n",
        "    def suggest_resolution():\n",
        "      pass\n",
        "\n",
        "  class Library:\n",
        "    def get_imports(requesting_library):\n",
        "      pass\n",
        "\n",
        "  class Downloader:\n",
        "    def fetch_bytes(url_or_path):\n",
        "      if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "          from urllib.request import urlopen \n",
        "          return urlopen(url_or_path) \n",
        "      return open(url_or_path, 'r')\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "\n",
        "# Need to clean up code, it's a mess\n",
        "global LAST_INIT\n",
        "global LAST_VRAM\n",
        "global LAST_MODE\n",
        "global LAST_MODEL_ID\n",
        "global LAST_ENABLE_NSFW_FILTER\n",
        "global LAST_DIFFUSERS_VERSION\n",
        "global INIT_IMAGE\n",
        "global INPAINT_IMAGE\n",
        "global MASK_IMAGE\n",
        "\n",
        "def clean_env():\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "def fetch_bytes(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        from urllib.request import urlopen \n",
        "        return urlopen(url_or_path) \n",
        "    return open(url_or_path, 'r')\n",
        "\n",
        "\n",
        "def patch_nsfw():\n",
        "  import shutil\n",
        "  import os\n",
        "  os.remove('/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py')\n",
        "  if ENABLE_NSFW_FILTER:\n",
        "    shutil.copyfile(f'/content/safety_checker.py', '/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py')\n",
        "  else:\n",
        "    shutil.copyfile(f'/content/safety_checker_patched.py', '/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def forward(self, x, context=None, mask=None):\n",
        "\n",
        "    import math\n",
        "    from torch import einsum\n",
        "    try:\n",
        "      from einops import rearrange\n",
        "    except ModuleNotFoundError:\n",
        "      !pip install einops\n",
        "      from einops import rearrange\n",
        "    import types\n",
        "    from diffusers.models.attention import CrossAttention\n",
        "    import torch\n",
        "    batch_size, sequence_length, dim = x.shape\n",
        "\n",
        "    h = self.heads\n",
        "\n",
        "    q = self.to_q(x)\n",
        "    context = context if context is not None else x\n",
        "    k = self.to_k(context)\n",
        "    v = self.to_v(context)\n",
        "    del context, x\n",
        "\n",
        "    q = self.reshape_heads_to_batch_dim(q)\n",
        "    k = self.reshape_heads_to_batch_dim(k)\n",
        "    v = self.reshape_heads_to_batch_dim(v)\n",
        "\n",
        "    r1 = torch.zeros(q.shape[0], q.shape[1], v.shape[2], device=q.device)\n",
        "\n",
        "    stats = torch.cuda.memory_stats(q.device)\n",
        "    mem_total = torch.cuda.get_device_properties(0).total_memory\n",
        "    mem_active = stats['active_bytes.all.current']\n",
        "    mem_free = mem_total - mem_active\n",
        "\n",
        "    mem_required = q.shape[0] * q.shape[1] * k.shape[1] * 4 * 2.5\n",
        "    steps = 1\n",
        "\n",
        "    if mem_required > mem_free:\n",
        "        steps = 2**(math.ceil(math.log(mem_required / mem_free, 2)))\n",
        "\n",
        "    slice_size = q.shape[1] // steps if (q.shape[1] % steps) == 0 else q.shape[1]\n",
        "    for i in range(0, q.shape[1], slice_size):\n",
        "        end = i + slice_size\n",
        "        s1 = einsum('b i d, b j d -> b i j', q[:, i:end], k)\n",
        "        s1 *= self.scale\n",
        "\n",
        "        s2 = s1.softmax(dim=-1)\n",
        "        del s1\n",
        "\n",
        "        r1[:, i:end] = einsum('b i j, b j d -> b i d', s2, v)\n",
        "        del s2\n",
        "\n",
        "    del q, k, v\n",
        "\n",
        "    r2 = rearrange(r1, '(b h) n d -> b n (h d)', h=h)\n",
        "    del r1\n",
        "\n",
        "    return self.to_out(r2)\n",
        "\n",
        "def optimize_attention(model):\n",
        "    import types\n",
        "    from diffusers.models.attention import CrossAttention\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, CrossAttention):\n",
        "            module.forward = types.MethodType(forward, module)\n",
        "\n",
        "def make_pipe():\n",
        "  # TODO: Cache pipes and clean this. Very messy right now\n",
        "  global LOW_VRAM_PATCH\n",
        "  global pipe\n",
        "  pipe = None\n",
        "  clean_env()\n",
        "  if MODE == \"IMG2IMG\":\n",
        "    if LOW_VRAM_PATCH:\n",
        "      try:\n",
        "        pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=True).to(\"cuda\")\n",
        "      except Exception:\n",
        "        !pip install transformers\n",
        "        # try:\n",
        "        #   with fetch_bytes('https://raw.githubusercontent.com/WASasquatch/easydiffusion/main/key.txt') as f:\n",
        "        #     key = f.read().decode('utf-8').split(':')\n",
        "        # except OSError as e:\n",
        "        #   print(e)\n",
        "        huggingface_username = 'x90'\n",
        "        huggingface_token = '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'\n",
        "        !echo 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 | huggingface-cli login\n",
        "        pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=True).to(\"cuda\")\n",
        "    else:\n",
        "      try:\n",
        "        pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, use_auth_token=True).to(\"cuda\")\n",
        "      except Exception:\n",
        "        !pip install transformers\n",
        "        # try:\n",
        "        #   with fetch_bytes('https://raw.githubusercontent.com/WASasquatch/easydiffusion/main/key.txt') as f:\n",
        "        #     key = f.read().decode('utf-8').split(':')\n",
        "        # except OSError as e:\n",
        "        #   print(e)\n",
        "        huggingface_username = 'x90'\n",
        "        huggingface_token = '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'\n",
        "        !echo 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 | huggingface-cli login\n",
        "        pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, use_auth_token=True).to(\"cuda\")\n",
        "  elif MODE == \"Inpainting\":\n",
        "    if LOW_VRAM_PATCH:\n",
        "      try:\n",
        "        pipe = StableDiffusionInpaintPipeline.from_pretrained(model_id, revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=True).to(\"cuda\")\n",
        "      except Exception:\n",
        "        !pip install transformers\n",
        "        # try:\n",
        "        #   with fetch_bytes('https://raw.githubusercontent.com/WASasquatch/easydiffusion/main/key.txt') as f:\n",
        "        #     key = f.read().decode('utf-8').split(':')\n",
        "        # except OSError as e:\n",
        "        #   print(e)\n",
        "        huggingface_username = 'x90'\n",
        "        huggingface_token = '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'\n",
        "        !echo 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 | huggingface-cli login\n",
        "        pipe = StableDiffusionInpaintPipeline.from_pretrained(model_id, revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=True).to(\"cuda\")\n",
        "    else:\n",
        "      try:\n",
        "        pipe = StableDiffusionInpaintPipeline.from_pretrained(model_id, use_auth_token=True).to(\"cuda\")\n",
        "      except Exception:\n",
        "        !pip install transformers\n",
        "        # try:\n",
        "        #   with fetch_bytes('https://raw.githubusercontent.com/WASasquatch/easydiffusion/main/key.txt') as f:\n",
        "        #     key = f.read().decode('utf-8').split(':')\n",
        "        # except OSError as e:\n",
        "        #   print(e)\n",
        "        huggingface_username = 'x90'\n",
        "        huggingface_token = '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'\n",
        "        !echo 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 | huggingface-cli login\n",
        "        pipe = StableDiffusionInpaintPipeline.from_pretrained(model_id, use_auth_token=True).to(\"cuda\")\n",
        "  elif LOW_VRAM_PATCH:\n",
        "      try:\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(model_id, revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=True).to(\"cuda\")\n",
        "      except Exception:\n",
        "        !pip install transformers\n",
        "        # try:\n",
        "        #   with fetch_bytes('https://raw.githubusercontent.com/WASasquatch/easydiffusion/main/key.txt') as f:\n",
        "        #     key = f.read().decode('utf-8').split(':')\n",
        "        # except OSError as e:\n",
        "        #   print(e)\n",
        "        huggingface_username = 'x90'\n",
        "        huggingface_token = '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'\n",
        "        !echo $huggingface_token | huggingface-cli login\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(model_id, revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=True).to(\"cuda\")\n",
        "        del pipe.vae.encoder\n",
        "  else:\n",
        "    try:\n",
        "      pipe = StableDiffusionPipeline.from_pretrained(model_id, use_auth_token=True).to(\"cuda\")\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      !pip install transformers\n",
        "      try:\n",
        "        with fetch_bytes('https://raw.githubusercontent.com/WASasquatch/easydiffusion/main/key.txt') as f:\n",
        "          key = f.read().decode('utf-8').split(':')\n",
        "      except OSError as e:\n",
        "        print(e)\n",
        "      huggingface_username = 'x90'\n",
        "      huggingface_token = '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'\n",
        "      !echo $huggingface_token | huggingface-cli login\n",
        "      pipe = StableDiffusionPipeline.from_pretrained(model_id, use_auth_token=True).to(\"cuda\")\n",
        "      del pipe.vae.encoder\n",
        "  if VRAM_OVER_SPEED:\n",
        "    print(\"Creating pipe optimizations\")\n",
        "    pipe.enable_attention_slicing()\n",
        "    optimize_attention(pipe.unet)\n",
        "\n",
        "def make_scheduler():\n",
        "  if SCHEDULER == 'default':\n",
        "    pipe.scheduler = PNDMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000, skip_prk_steps=True)\n",
        "  elif SCHEDULER == 'pndm':\n",
        "    pipe.scheduler = PNDMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000, skip_prk_steps=True)\n",
        "  elif SCHEDULER == 'k-lms':\n",
        "    pipe.scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
        "  elif SCHEDULER == 'ddim':\n",
        "    pipe.scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
        "\n",
        "\n",
        "def make_image():\n",
        "  # Clean this\n",
        "  gen_seed = torch.Generator(\"cuda\").manual_seed(SEED)\n",
        "  global pipe\n",
        "  global INIT_IMAGE\n",
        "  global INPAINT_IMAGE\n",
        "  global MASK_IMAGE\n",
        "  if MODE == \"Inpainting\":\n",
        "    print(\"Inpainting Mode\")\n",
        "    print(\"Init Image (automatically resized to match user input)\")\n",
        "    INPAINT_IMAGE = INPAINT_IMAGE.resize((WIDTH, HEIGHT))\n",
        "    MASK_IMAGE = MASK_IMAGE.resize((WIDTH, HEIGHT))\n",
        "    display(INPAINT_IMAGE)\n",
        "    display(MASK_IMAGE)\n",
        "\n",
        "    inpaint_image = INPAINT_IMAGE\n",
        "    mask_image = MASK_IMAGE\n",
        "    # init_image = INIT_IMAGE.convert(\"RGB\"))\n",
        "    # \n",
        "    if SCHEDULER == 'ddim':\n",
        "      try:\n",
        "        image = pipe(PROMPT, init_image=inpaint_image, mask_image=mask_image, strength=INPAINT_STRENGTH, guidance_scale=SCALE, eta=DDIM_ETA, generator=gen_seed)[\"sample\"][0]\n",
        "      except IndexError:\n",
        "        try:\n",
        "          image = pipe(PROMPT, init_image=inpaint_image, mask_image=mask_image, strength=INPAINT_STRENGTH, guidance_scale=SCALE, eta=DDIM_ETA, generator=gen_seed)[\"sample\"][0]\n",
        "        except UnboundLocalError or NameError:\n",
        "          make_pipe()\n",
        "          make_scheduler()\n",
        "          make_image()\n",
        "    else:\n",
        "      try:\n",
        "        image = pipe(PROMPT, init_image=inpaint_image, mask_image=mask_image, strength=INPAINT_STRENGTH, guidance_scale=SCALE, generator=gen_seed)[\"sample\"][0]\n",
        "      except IndexError:\n",
        "        try:\n",
        "          image = pipe(PROMPT, init_image=inpaint_image, mask_image=mask_image, strength=INPAINT_STRENGTH, guidance_scale=SCALE, generator=gen_seed)[\"sample\"][0]\n",
        "        except UnboundLocalError or NameError:\n",
        "          make_pipe()\n",
        "          make_scheduler()\n",
        "          make_image()\n",
        "      except UnboundLocalError or NameError:\n",
        "        make_pipe()\n",
        "        make_scheduler()\n",
        "        make_image\n",
        "      except RuntimeError as e:\n",
        "        print(e)\n",
        "        clean_env()\n",
        "        try:\n",
        "          image = None\n",
        "        except Exception:\n",
        "          pass\n",
        "        raise SystemExit('\\33[33mUsing too much VRAM, lower your settings.\\33[0m')\n",
        "  elif MODE == \"IMG2IMG\":\n",
        "    print(\"Init Mode\")\n",
        "    print(\"Init Image (automatically resized to match user input)\")\n",
        "    INIT_IMAGE = INIT_IMAGE.resize((WIDTH, HEIGHT))\n",
        "    display(INIT_IMAGE)\n",
        "    def preprocess(image):\n",
        "      w, h = image.size\n",
        "      w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
        "      image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
        "      image = np.array(image).astype(np.float32) / 255.0\n",
        "      image = image[None].transpose(0, 3, 1, 2)\n",
        "      image = torch.from_numpy(image)\n",
        "      return 2.*image - 1.\n",
        "\n",
        "    init_image = preprocess(INIT_IMAGE.convert(\"RGB\"))\n",
        "    # init_image = INIT_IMAGE.convert(\"RGB\"))\n",
        "    if SCHEDULER == 'ddim':\n",
        "      try:\n",
        "        image = pipe(PROMPT, num_inference_steps=STEPS, init_image=init_image, strength=INIT_STRENGTH, guidance_scale=SCALE, eta=DDIM_ETA, generator=gen_seed)[\"sample\"][0]\n",
        "      except IndexError:\n",
        "        try:\n",
        "          image = pipe(PROMPT, num_inference_steps=STEPS, init_image=init_image, strength=INIT_STRENGTH, guidance_scale=SCALE, eta=DDIM_ETA, generator=gen_seed)[\"sample\"][0]\n",
        "        except UnboundLocalError or NameError:\n",
        "          make_pipe()\n",
        "          make_scheduler()\n",
        "          make_image()\n",
        "    else:\n",
        "      try:\n",
        "        image = pipe(prompt=PROMPT, num_inference_steps=STEPS, init_image=init_image, strength=INIT_STRENGTH, guidance_scale=SCALE, generator=gen_seed)[\"sample\"][0]\n",
        "      except IndexError:\n",
        "        try:\n",
        "          image = pipe(prompt=PROMPT, num_inference_steps=STEPS, init_image=init_image, strength=INIT_STRENGTH, guidance_scale=SCALE, generator=gen_seed)[\"sample\"][0]\n",
        "        except UnboundLocalError or NameError:\n",
        "          make_pipe()\n",
        "          make_scheduler()\n",
        "          make_image()\n",
        "      except UnboundLocalError or NameError:\n",
        "        make_pipe()\n",
        "        make_scheduler()\n",
        "        make_image\n",
        "      except RuntimeError as e:\n",
        "        print(e)\n",
        "        clean_env()\n",
        "        try:\n",
        "          image = None\n",
        "        except Exception:\n",
        "          pass\n",
        "        raise SystemExit('\\33[33mUsing too much VRAM, lower your settings.\\33[0m')\n",
        "  else:\n",
        "    if SCHEDULER == 'ddim':\n",
        "      try:\n",
        "        image = pipe(PROMPT, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, eta=DDIM_ETA, generator=gen_seed)[\"sample\"][0]\n",
        "      except IndexError:\n",
        "        try:\n",
        "          image = pipe(PROMPT, num_inference_steps=STEPS-1, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, eta=DDIM_ETA, generator=gen_seed)[\"sample\"][0]\n",
        "        except UnboundLocalError or NameError:\n",
        "          make_pipe()\n",
        "          make_scheduler()\n",
        "          make_image()\n",
        "    else:\n",
        "      try:\n",
        "        image = pipe(PROMPT, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, generator=gen_seed)[\"sample\"][0]\n",
        "      except IndexError:\n",
        "        try:\n",
        "          image = pipe(PROMPT, num_inference_steps=STEPS-1, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, generator=gen_seed)[\"sample\"][0]\n",
        "        except UnboundLocalError or NameError:\n",
        "          make_pipe()\n",
        "          make_scheduler()\n",
        "          make_image()\n",
        "      except UnboundLocalError or NameError or TypeError:\n",
        "        make_pipe()\n",
        "        make_scheduler()\n",
        "        make_image\n",
        "      except RuntimeError as e:\n",
        "        print(e)\n",
        "        clean_env()\n",
        "        try:\n",
        "          image = None\n",
        "        except Exception:\n",
        "          pass\n",
        "        raise SystemExit('\\33[33mUsing too much VRAM, lower your settings.\\33[0m')\n",
        "  return image\n",
        "\n",
        "\n",
        "def diffusers_install():\n",
        "  if CLEAR_SETUP_LOG: from IPython.display import clear_output; clear_output()\n",
        "  try:\n",
        "    with fetch_bytes('https://raw.githubusercontent.com/WASasquatch/easydiffusion/main/key.txt') as f:\n",
        "      key = f.read().decode('utf-8').split(':')\n",
        "  except OSError as e:\n",
        "    print(e)\n",
        "    \n",
        "  huggingface_username = 'x90'\n",
        "  huggingface_token = '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'\n",
        "  global LAST_INIT, LAST_VRAM, LAST_ENABLE_NSFW_FILTER, LAST_MODEL_ID, LAST_DIFFUSERS_VERSION\n",
        "  LAST_MODE = MODE\n",
        "  LAST_VRAM = LOW_VRAM_PATCH\n",
        "  LAST_ENABLE_NSFW_FILTER = ENABLE_NSFW_FILTER\n",
        "  LAST_MODEL_ID = MODEL_ID\n",
        "  LAST_DIFFUSERS_VERSION = DIFFUSERS_VERSION\n",
        "  try: \n",
        "    !git lfs install\n",
        "    !GIT_LFS_SKIP_SMUDGE=0\n",
        "    # This will take a while\n",
        "    !pip install transformers\n",
        "    !git lfs clone https://$huggingface_username:$huggingface_token@huggingface.co/$MODEL_ID\n",
        "    if DIFFUSERS_VERSION == 'latest':\n",
        "      !pip install -U git+https://github.com/huggingface/diffusers.git\n",
        "    else:\n",
        "      !pip install -U git+https://github.com/huggingface/diffusers.git@$DIFFUSERS_VERSION\n",
        "\n",
        "    # Back up original NSFW file\n",
        "    !cp /usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py /content/safety_checker.py\n",
        "    !cp /content/safety_checker.py /content/safety_checker_patched.py\n",
        "    with open(f'/content/safety_checker_patched.py','r') as unpatched_file:\n",
        "      patch = unpatched_file.read().replace('for idx, has_nsfw_concept in enumerate(has_nsfw_concepts):','#for idx, has_nsfw_concept in enumerate(has_nsfw_concepts):').replace('if has_nsfw_concept:','# if has_nsfw_concept:').replace('images[idx] = np.zeros(images[idx].shape)  # black image', '# images[idx] = np.zeros(images[idx].shape)  # black image').replace(\"Potential NSFW content was detected in one or more images. A black image will be returned instead.\",\"Potential NSFW content was detected in one or more images. It's patched out, no actions were taken.\").replace(\" Try again with a different prompt and/or seed.\",\"\")\n",
        "    with open(f'/content/safety_checker_patched.py','w') as file:\n",
        "      file.write(patch)\n",
        "    patch_nsfw()\n",
        "    \n",
        "    # make sure you're logged in with `huggingface-cli login`\n",
        "\n",
        "    !mkdir diffusers_output\n",
        "    !pip install pytorch-pretrained-bert\n",
        "    !pip install spacy ftfy\n",
        "    !python -m spacy download en\n",
        "    !pip install scipy\n",
        "    !echo $huggingface_token | huggingface-cli login\n",
        "  except OSError as e:\n",
        "    raise e\n",
        "  except BaseException as e:\n",
        "    raise e\n",
        "  finally:\n",
        "    if CLEAR_SETUP_LOG: from IPython.display import clear_output; clear_output()\n",
        "    print(\"Setup complete.\")\n",
        "    try:\n",
        "      from diffusers.schedulers import PNDMScheduler, LMSDiscreteScheduler, DDIMScheduler, DDPMScheduler\n",
        "      from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, StableDiffusionInpaintPipeline\n",
        "    except ModuleNotFoundError or ImportError:\n",
        "      diffusers_install()\n",
        "      from diffusers.schedulers import PNDMScheduler, LMSDiscreteScheduler, DDIMScheduler, DDPMScheduler\n",
        "      from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, StableDiffusionInpaintPipeline\n",
        "  if CLEAR_SETUP_LOG: from IPython.display import clear_output; clear_output()\n",
        "\n",
        "\n",
        "def GFPGAN_install():\n",
        "  if CLEAR_SETUP_LOG: from IPython.display import clear_output; clear_output()\n",
        "  if not os.path.exists('/content/GFPGAN'):\n",
        "    !git clone https://github.com/TencentARC/GFPGAN.git\n",
        "    %cd GFPGAN\n",
        "    !pip install basicsr\n",
        "    !pip install facexlib\n",
        "    !pip install -r requirements.txt\n",
        "    !python setup.py develop\n",
        "    !pip install realesrgan  \n",
        "    !wget https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth -P experiments/pretrained_models\n",
        "    %cd /content/\n",
        "  if CLEAR_SETUP_LOG: from IPython.display import clear_output; clear_output()\n",
        "    \n",
        "\n",
        "def ESRGAN_install():\n",
        "  if CLEAR_SETUP_LOG: from IPython.display import clear_output; clear_output()\n",
        "  if not os.path.exists('/content/Real-ESRGAN'):\n",
        "    !git clone https://github.com/sberbank-ai/Real-ESRGAN\n",
        "    !pip install -r Real-ESRGAN/requirements.txt\n",
        "    !wget https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x2.pth -O /content/Real-ESRGAN/weights/RealESRGAN_x2.pth\n",
        "    !wget https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x4.pth -O /content/Real-ESRGAN/weights/RealESRGAN_x4.pth\n",
        "    !wget https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x8.pth -O /content/Real-ESRGAN/weights/RealESRGAN_x8.pth\n",
        "  %cd Real-ESRGAN\n",
        "  from realesrgan import RealESRGAN\n",
        "  clear_output()\n",
        "  device = torch.device('cuda')\n",
        "  global UPSCALE_AMOUNT\n",
        "  if not os.path.exists(f'/content/Real-ESRGAN/weights/RealESRGAN_x{UPSCALE_AMOUNT}.pth'):\n",
        "    def closest_value(input_list, input_value):\n",
        "      difference = lambda input_list : abs(input_list - input_value)\n",
        "      res = min(input_list, key=difference)\n",
        "      return res\n",
        "    nearest_value = closest_value([2,4,8],UPSCALE_AMOUNT)\n",
        "    print(f'For Real-ESRGAN upscaling only 2, 4, and 8 are supported. Choosing the nearest Value: {nearest_value}')\n",
        "    UPSCALE_AMOUNT = nearest_value\n",
        "\n",
        "  model = RealESRGAN(device, scale = UPSCALE_AMOUNT)\n",
        "  model.load_weights(f'/content/Real-ESRGAN/weights/RealESRGAN_x{UPSCALE_AMOUNT}.pth')\n",
        "  %cd /content/\n",
        "  if CLEAR_SETUP_LOG: from IPython.display import clear_output; clear_output()\n",
        "\n",
        "def CodeFormer_install():\n",
        "  if CLEAR_SETUP_LOG: from IPython.display import clear_output; clear_output()\n",
        "  if not os.path.exists('/content/CodeFormer'):\n",
        "    %cd /content\n",
        "    !git clone https://github.com/sczhou/CodeFormer.git\n",
        "    %cd CodeFormer\n",
        "    !pip install -r requirements.txt\n",
        "    !python basicsr/setup.py develop\n",
        "    !python scripts/download_pretrained_models.py facelib\n",
        "    !python scripts/download_pretrained_models.py CodeFormer\n",
        "    !mkdir temp\n",
        "    !mkdir results\n",
        "    %cd /content/\n",
        "  if CLEAR_SETUP_LOG: from IPython.display import clear_output; clear_output()\n",
        "\n",
        "# If settings changed, repopulate\n",
        "def populate():\n",
        "  global LAST_INIT\n",
        "  global LAST_MODE\n",
        "  global LAST_VRAM\n",
        "  global LAST_ENABLE_NSFW_FILTER\n",
        "  global LAST_MODEL_ID\n",
        "  global LAST_DIFFUSERS_VERSION\n",
        "  global pipe\n",
        "  pipe = None\n",
        "  clean_env()\n",
        "  if DIFFUSERS_VERSION != LAST_DIFFUSERS_VERSION:\n",
        "    !yes | pip uninstall diffusers\n",
        "  if LAST_MODEL_ID != MODEL_ID or DIFFUSERS_VERSION != LAST_DIFFUSERS_VERSION:\n",
        "    print(\"Setting up for new model..\")\n",
        "    diffusers_install()\n",
        "  if CLEAR_SETUP_LOG: from IPython.display import clear_output; clear_output()\n",
        "  print(\"Patching NSFW...\")\n",
        "  patch_nsfw()\n",
        "  if CLEAR_SETUP_LOG: from IPython.display import clear_output; clear_output()\n",
        "  print(\"Making Pipe...\")\n",
        "  make_pipe()\n",
        "  if CLEAR_SETUP_LOG: from IPython.display import clear_output; clear_output()\n",
        "  print(\"Injecting scheduler...\")\n",
        "  make_scheduler()\n",
        "  if CLEAR_SETUP_LOG: from IPython.display import clear_output; clear_output()\n",
        "  LAST_MODEL_ID = MODEL_ID\n",
        "  LAST_MODE = MODE\n",
        "  LAST_VRAM = LOW_VRAM_PATCH\n",
        "  LAST_ENABLE_NSFW_FILTER = ENABLE_NSFW_FILTER\n",
        "\n",
        "\n",
        "# Diffuse Function\n",
        "def upscale(image):\n",
        "    try:\n",
        "      from realesrgan import RealESRGAN\n",
        "    except ModuleNotFoundError:\n",
        "      if not os.path.exists('/content/Real-ESRGAN'):\n",
        "        ESRGAN_install()\n",
        "        %cd /content/Real-ESRGAN\n",
        "        from realesrgan import RealESRGAN\n",
        "        %cd /content\n",
        "    device = torch.device('cuda')\n",
        "    model = RealESRGAN(device, scale = UPSCALE_AMOUNT)\n",
        "    try:\n",
        "      model.load_weights(f'/content/Real-ESRGAN/weights/RealESRGAN_x{UPSCALE_AMOUNT}.pth')\n",
        "    except FileNotFoundError:\n",
        "      !wget https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x2.pth -O /content/Real-ESRGAN/weights/RealESRGAN_x2.pth\n",
        "      !wget https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x4.pth -O /content/Real-ESRGAN/weights/RealESRGAN_x4.pth\n",
        "      !wget https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x8.pth -O /content/Real-ESRGAN/weights/RealESRGAN_x8.pth\n",
        "      model.load_weights(f'/content/Real-ESRGAN/weights/RealESRGAN_x{UPSCALE_AMOUNT}.pth')\n",
        "    sr_image = model.predict(np.array(image))\n",
        "    return sr_image\n",
        "\n",
        "\n",
        "# Diffusion start\n",
        "def diffuse_run():\n",
        "  # Can be cleaned quite a bit\n",
        "    global SEED\n",
        "    global pipe\n",
        "    if ORIG_SEED == 0:\n",
        "      if iteration is 0:\n",
        "        SEED = random.randint(0,sys.maxsize)\n",
        "      if iteration is not 0 and not KEEP_SEED:\n",
        "        SEED += 1\n",
        "    else:\n",
        "      if iteration > 0 and not KEEP_SEED:\n",
        "        SEED += 1\n",
        "    gen_seed = torch.Generator(\"cuda\").manual_seed(SEED)\n",
        "    epoch_time = int(time.time())\n",
        "    print(f'Seed: {SEED}, Scale: {SCALE}, Steps: {STEPS}')\n",
        "    clean_env()\n",
        "    \n",
        "    try:\n",
        "      image = make_image()\n",
        "    except NameError or TypeError:\n",
        "      make_pipe()\n",
        "      make_scheduler()\n",
        "      image = make_image()\n",
        "    except RuntimeError as e:\n",
        "      print(e)\n",
        "      clean_env()\n",
        "      try:\n",
        "        image = None\n",
        "      except Exception:\n",
        "        pass\n",
        "      raise SystemExit('\\33[33mUsing too much VRAM, lower your settings.\\33[0m')\n",
        "    display(image)\n",
        "    filename = f'{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}.png'\n",
        "    filedir = f'{OUTDIR}/{filename}'\n",
        "    image.save(filedir)\n",
        "    clean_env()\n",
        "    if IMAGE_UPSCALER == \"GFPGAN\":\n",
        "      print('Face Enhancing and Upscaling... ')\n",
        "      %cd GFPGAN\n",
        "      try:\n",
        "        !python /content/GFPGAN/inference_gfpgan.py -i $filedir -o $OUTDIR -v 1.3 -s $UPSCALE_AMOUNT --bg_upsampler realesrgan\n",
        "      except FileNotFoundError:\n",
        "        ESRGAN_install()\n",
        "      if not SKIP_PREVIEW:\n",
        "        display(PIL.Image.open(f'{OUTDIR}/restored_imgs/{filename}'))\n",
        "      %cd ..\n",
        "      print(f'Moving enhanced image to {OUTDIR}')\n",
        "      old_filedir = filedir\n",
        "      filedir = f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png'\n",
        "      shutil.move(f'{OUTDIR}/restored_imgs/{filename}', filedir)\n",
        "      try:\n",
        "        if DELETE_ORIGINALS:\n",
        "          os.remove(old_filedir)\n",
        "      except FileNotFoundError:\n",
        "        pass\n",
        "      clean_env()\n",
        "    if IMAGE_UPSCALER == \"Enhanced Real-ESRGAN\":\n",
        "      print('Upscaling... ')\n",
        "      sr_image = upscale(image)\n",
        "      if not SKIP_PREVIEW:\n",
        "        display(sr_image)\n",
        "      old_filedir = filedir\n",
        "      try:\n",
        "        filedir = f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{rand_num}_upscaled_{UPSCALE_AMOUNT}.png'\n",
        "      except NameError:\n",
        "        filedir = f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}_upscaled_{UPSCALE_AMOUNT}.png'\n",
        "      sr_image.save(filedir)\n",
        "      if DELETE_ORIGINALS:\n",
        "        os.remove(old_filedir)\n",
        "      clean_env()\n",
        "    if IMAGE_UPSCALER == \"GFPGAN + Enhanced ESRGAN\":\n",
        "      print('Face Enhancing... ')\n",
        "      %cd GFPGAN\n",
        "      try:\n",
        "        !python /content/GFPGAN/inference_gfpgan.py -i $filedir -o $OUTDIR -v 1.3 -s 1 --bg_upsampler realesrgan\n",
        "      except FileNotFoundError:\n",
        "        ESRGAN_install()\n",
        "      if not SKIP_PREVIEW:\n",
        "        display(PIL.Image.open(f'{OUTDIR}/restored_imgs/{filename}'))\n",
        "      %cd ..\n",
        "      shutil.copy(f'{OUTDIR}/restored_imgs/{filename}', f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "      clean_env()\n",
        "      print('Upscaling... ')\n",
        "      sr_image = upscale(PIL.Image.open(f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png'))\n",
        "      if not SKIP_PREVIEW:\n",
        "        display(sr_image)\n",
        "      old_filedir = filedir\n",
        "      try:\n",
        "        filedir = f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png'\n",
        "      except NameError:\n",
        "        filedir = f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png'\n",
        "      sr_image.save(filedir)\n",
        "      if DELETE_ORIGINALS:\n",
        "        try:\n",
        "          os.remove(old_filedir)\n",
        "        except Exception as e:\n",
        "          print(e)\n",
        "      clean_env()\n",
        "    if IMAGE_UPSCALER == \"CodeFormer\":\n",
        "      print(\"Face enhancing and Upscaling... \")\n",
        "      # It was behaving weird, hence why I am doing this the weird way\n",
        "      try:\n",
        "        !rm rm /content/CodeFormer/temp/*\n",
        "        !cp $filedir /content/CodeFormer/temp/\n",
        "      except Exception as e:\n",
        "        os.makedirs('/content/CodeFormer/temp/')\n",
        "        !cp $filedir /content/CodeFormer/temp/\n",
        "      %cd /content/CodeFormer/\n",
        "      !python inference_codeformer.py --w $CODEFORMER_FIDELITY --test_path /content/CodeFormer/temp --upscale $UPSCALE_AMOUNT --bg_upsampler realesrgan\n",
        "      old_filedir = filedir\n",
        "      filedir = f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png'\n",
        "      shutil.copyfile(f'/content/CodeFormer/results/temp_{CODEFORMER_FIDELITY}/final_results/{filename}', filedir)\n",
        "      os.remove(f'/content/CodeFormer/temp/{filename}')\n",
        "      if DELETE_ORIGINALS:\n",
        "        try:\n",
        "          os.remove(old_filedir)\n",
        "        except Exception as e:\n",
        "          print(e)\n",
        "      %cd /content\n",
        "      if not SKIP_PREVIEW:\n",
        "        display(PIL.Image.open(filedir))\n",
        "      clean_env()\n",
        "    if IMAGE_UPSCALER == \"CodeFormer + Enhanced ESRGAN\":\n",
        "      print(\"Face enhancing... \")\n",
        "      try:\n",
        "        !rm /content/CodeFormer/temp/*\n",
        "        !cp $filedir /content/CodeFormer/temp/\n",
        "      except Exception as e:\n",
        "        os.makedirs('/content/CodeFormer/temp/')\n",
        "        !cp $filedir /content/CodeFormer/temp/\n",
        "      %cd /content/CodeFormer/\n",
        "      !python inference_codeformer.py --w $CODEFORMER_FIDELITY --test_path /content/CodeFormer/temp --upscale 1 --bg_upsampler realesrgan\n",
        "      os.remove(f'/content/CodeFormer/temp/{filename}')\n",
        "      shutil.copyfile(f'/content/CodeFormer/results/temp_{CODEFORMER_FIDELITY}/final_results/{filename}', f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "      %cd /content\n",
        "      if not SKIP_PREVIEW:\n",
        "        display(PIL.Image.open(f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png'))\n",
        "      clean_env()\n",
        "      print(\"Upscaling... \")\n",
        "      sr_image = upscale(PIL.Image.open(f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png'))\n",
        "      if not SKIP_PREVIEW:\n",
        "        display(sr_image)\n",
        "      old_filedir = filedir\n",
        "      filedir = f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png'\n",
        "      sr_image.save(filedir)\n",
        "      if DELETE_ORIGINALS:\n",
        "        try:\n",
        "          os.remove(old_filedir)\n",
        "        except Exception as e:\n",
        "          print(e)\n",
        "      clean_env()\n",
        "    if int(SHARPEN_AMOUNT) != 0:\n",
        "      def sharpenImage(image, samples=1):\n",
        "        im = image\n",
        "        for i in range(samples):\n",
        "            im = im.filter(ImageFilter.SHARPEN)\n",
        "        return im\n",
        "      print(f\"Sharpening diffusion result with {SHARPEN_AMOUNT} passes.\\n\")\n",
        "      image = sharpenImage(PIL.Image.open(filedir), SHARPEN_AMOUNT)\n",
        "      if not SKIP_PREVIEW:\n",
        "        display(image)\n",
        "      old_filedir = filedir\n",
        "      filedir = f'{filedir.strip(\".png\")}_sharpened_{SHARPEN_AMOUNT}.png'\n",
        "      image.save(filedir)\n",
        "      if DELETE_ORIGINALS:\n",
        "        try:\n",
        "          os.remove(old_filedir)\n",
        "        except Exception as e:\n",
        "          print(e)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "\n",
        "  stop_its = False\n",
        "  %cd /content/\n",
        "  #@title Render Images\n",
        "  MODE = \"PROMPT\" #@param [\"PROMPT\",\"IMG2IMG\",\"Inpainting\"]\n",
        "  #@markdown `MODE` Select what mode you want to use <br>\n",
        "  PROMPT = \"A young woman wearing a hat, greg rutkowski, artgerm, trending on artstation, cinematic animation still, by lois van baarle, ilya kuvshinov, metahuman\" #@param {type:'string'}\n",
        "  #@markdown `PROMPT` The text prompt that is needed for all modes <br>\n",
        "  PROMPT_FILE = '' #@param {type: 'string'}\n",
        "  #@markdown `PROMPT_FILE` is a text file that contains a prompt per line. <br>\n",
        "\n",
        "  #@markdown ---\n",
        "  \n",
        "  #@markdown <b>Init Image Setup (IMG2IMG)</b><br>\n",
        "  #@markdown Still a work in progress, might be buggy<br>\n",
        "\n",
        "  INIT_IMAGE = \"https://raw.githubusercontent.com/dblunk88/txt2imghd/master/character_with_hat.jpg\" #@param {type: 'string'}\n",
        "  #@markdown `INIT_IMAGE`: Can be a local file, URL, or empty (to make your own in colab)<br>\n",
        "  INIT_STRENGTH = 0.92 #@param{type:\"slider\", min:0.01, max:1, step:0.01}\n",
        "  #@markdown `INIT_STRENGTH`: The <B>LOWER</B> this values is, the more strength the init file has on the final output\n",
        "\n",
        "  #@markdown ---\n",
        "\n",
        "  #@markdown <b>INPAINT Setup</b><br>\n",
        "  #@markdown Still a work in progress, might be buggy<br>\n",
        "  #@markdown `INPAINT_IMAGE` can be a local file or URL<br>\n",
        "  #@markdown `MASK_IMAGE` can be a local file, URL, or empty. If empty, you get to draw your mask within the colab\n",
        "  INPAINT_IMAGE = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\" #@param {type:'string'}\n",
        "  MASK_IMAGE = \"\" #@param {type:'string'}\n",
        "  INPAINT_STRENGTH = 0.96 #@param {type:\"slider\", min:0, max:1, step:0.01} \n",
        "\n",
        "  #@markdown ---\n",
        "\n",
        "  #@markdown <b>GENERAL SETUP</b><br>\n",
        "  #@markdown Most of these settings are used for all modes\n",
        "  STEPS = 200 #@param {type:\"slider\", min:5, max:500, step:5} \n",
        "  SCHEDULER = 'default' #@param [\"default\", \"pndm\", \"k-lms\", \"ddim\"]\n",
        "  DDIM_ETA = 0.77 #@param {type:\"slider\", min:0, max:1, step:0.01} \n",
        "  #@markdown Getting good results with `ddim` and `DDIM_ETA` at 0.9\n",
        "  #@markdown Diffusion steps determine the quality of the final image\n",
        "  SEED = 0 #@param {type:'integer'}\n",
        "  #@markdown The seed used for the generation. Leave at `0` for random.\n",
        "  KEEP_SEED = False #@param {type:'boolean'}\n",
        "  #@markdown Will force the program to keep the same seed throughout the iterations.\n",
        "  NUM_ITERS = 3 #@param {type:\"slider\", min:1, max:100, step:1} \n",
        "  RUN_FOREVER = False #@param {type:\"boolean\"}\n",
        "  #@markdown Number of iterations for a given prompt.\n",
        "  WIDTH = 512 #@param {type:\"slider\", min:256, max:4096, step:64}\n",
        "  HEIGHT = 512 #@param {type:\"slider\", min:256, max:4096, step:64}\n",
        "  SCALE = 13.8 #@param {type:\"slider\", min:0, max:25, step:0.1}\n",
        "  #@markdown The CFG scale determines how closely a generation follows the prompt, or improvisation. Higher values will try to adhear to your prompt.<br>\n",
        "  PRECISION = \"autocast\" #@param [\"full\",\"autocast\"]\n",
        "  #@markdown If you're using the `low VRAM patch` you <b>HAVE</b> to use `autocast`<br>\n",
        "  SAVE_PROMPT_DETAILS = True #@param {type:\"boolean\"}\n",
        "  USE_DRIVE_FOR_PICS = True #@param {type:\"boolean\"}\n",
        "  DRIVE_PIC_DIR = \"AI_PICS\" #@param {type:\"string\"}\n",
        "\n",
        "  #@markdown ---\n",
        "\n",
        "  #@markdown <b>POST PROCESSING</b><br>\n",
        "  #@markdown `IMAGE_UPSCALER`: May not work with the Tesla T4. GFP is pretty good at faces and the enhanced Real-ESRGAN is a pretty good uspcaler. If both is selected, then GFPGAN will act as a face enhancer and Real-ESRGAN will act as the upscaler. Same applies with Codeformer, which seems to be a little more mild than GFP.<br>Recommendations: GFPGAN if you only have faces or People. ESRGAN if you have no people in your prompt. Both if you have people and other things in your prompt<br>Note: ESRGAN only support 2x, 4x, and 8x, if any other value is selected, it will pick the nearest value\n",
        "  IMAGE_UPSCALER = \"CodeFormer + Enhanced ESRGAN\" #@param [\"None\",\"GFPGAN\",\"Enhanced Real-ESRGAN\", \"GFPGAN + Enhanced ESRGAN\", \"CodeFormer\", \"CodeFormer + Enhanced ESRGAN\"]\n",
        "  UPSCALE_AMOUNT = 2 #@param {type:\"raw\"}\n",
        "  #@markdown `CODEFORMER_FIDELITY`: Only applies if the upscaler includes Codeformer. Balance the quality (lower number) and fidelity (higher number)<br>\n",
        "  CODEFORMER_FIDELITY = 0.8 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "  #@markdown `SHARPEN_AMOUNT`: Select 0 to turn it off\n",
        "  SHARPEN_AMOUNT = 1 #@param{type:'slider', min:0, max:3, step:1}\n",
        "  DELETE_ORIGINALS = False #@param{type:'boolean'}\n",
        "  #@markdown `SKIP_PREVIEW`: Clicking this might help with connection issues (especially on mobile). It will only show the original result, not the improvements\n",
        "  SKIP_PREVIEW = False #@param{type:'boolean'}\n",
        "\n",
        "  #@markdown ---\n",
        "\n",
        "  #@markdown <b>SETUP</b><br>\n",
        "  MODEL_ID = \"CompVis/stable-diffusion-v1-4\" #@param [\"CompVis/stable-diffusion-v1-4\", \"CompVis/stable-diffusion-v1-3\",\"CompVis/stable-diffusion-v1-2\",\"CompVis/stable-diffusion-v1-1\",\"hakurei/waifu-diffusion\"]\n",
        "  model_id = MODEL_ID\n",
        "  DIFFUSERS_VERSION = 'latest' #@param [\"latest\",\"f3937bc8f3667772c9f1428b66f0c44b6087b04d\"]\n",
        "  LOW_VRAM_PATCH = False #@param {type:\"boolean\"}\n",
        "  #@markdown `LOW_VRAM_PATCH`: Click this if you have CUDA out of memory errors with low settings. If you check this you may be tied to using this setting until the next session restart since it patches various files. <br> This should also speed up iterations but could output lower quality content<br>\n",
        "  \n",
        "  VRAM_OVER_SPEED = True #@param {type:\"boolean\"}\n",
        "  #@markdown `VRAM_OVER_SPEED`: Some more optimizations. This option will prioritize lower VRAM usage over overall speed. Haven't tested this extensively but this is a big boi button. Crank that resolution up in autocast to see the effects<br>\n",
        "  \n",
        "  ENABLE_NSFW_FILTER = False #@param {type:\"boolean\"}\n",
        "  #@markdown `ENABLE_NSFW_FILTER`: Will ENABLE the NSFW filter. If you want uncensored results, do not click that. Needs a session restart as of right now if changed (fixing this)<br>\n",
        "  \n",
        "  CLEAR_SETUP_LOG = True #@param{type: 'boolean'}\n",
        "  #markdown Clear the setup log after installation compeltes.\n",
        "\n",
        "  #@markdown ---\n",
        "\n",
        "  ##@markdown <b>Advanced Options</b><br>\n",
        "  ##@markdown If you don't know what you are doing, do NOT touch this<br>\n",
        "\n",
        "\n",
        "  #     pipe.scheduler = PNDMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000, skip_prk_steps=True)\n",
        "  # elif SCHEDULER == 'pndm':\n",
        "  #   pipe.scheduler = PNDMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000, skip_prk_steps=True)\n",
        "  # elif SCHEDULER == 'k-lms':\n",
        "  #   pipe.scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
        "  # elif SCHEDULER == 'ddim':\n",
        "  #   pipe.scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
        "\n",
        "  \n",
        "  # Need to clean up imports\n",
        "  try:\n",
        "    import os, torch, gc\n",
        "  except ValueError:\n",
        "    !yes | pip uninstall numpy\n",
        "    !pip install -U numpy\n",
        "    import os, torch, gc\n",
        "  from PIL import Image\n",
        "  import random, time, shutil, sys\n",
        "  from contextlib import contextmanager, nullcontext\n",
        "  from torch import autocast\n",
        "  from IPython.display import clear_output\n",
        "  import numpy as np\n",
        "  import PIL.Image\n",
        "  import PIL\n",
        "  from PIL import ImageFilter\n",
        "  # Google Drive\n",
        "  if USE_DRIVE_FOR_PICS and not os.path.exists('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "  if USE_DRIVE_FOR_PICS and not os.path.exists(f'/content/drive/MyDrive/{DRIVE_PIC_DIR}'):\n",
        "    !mkdir /content/drive/MyDrive/$DRIVE_PIC_DIR\n",
        "  if USE_DRIVE_FOR_PICS:\n",
        "    OUTDIR = f'/content/drive/MyDrive/{DRIVE_PIC_DIR}'\n",
        "  else:\n",
        "    OUTDIR = '/content/diffusers_output'\n",
        "  try:\n",
        "    os.makedirs(OUTDIR)\n",
        "  except FileExistsError:\n",
        "    pass\n",
        "\n",
        "  try:\n",
        "    from diffusers.schedulers import PNDMScheduler, LMSDiscreteScheduler, DDIMScheduler, DDPMScheduler\n",
        "    from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, StableDiffusionInpaintPipeline\n",
        "  except ModuleNotFoundError or ImportError:\n",
        "    diffusers_install()\n",
        "    from diffusers.schedulers import PNDMScheduler, LMSDiscreteScheduler, DDIMScheduler, DDPMScheduler\n",
        "    from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, StableDiffusionInpaintPipeline\n",
        "  # Decide precision and set variables\n",
        "  precision_scope = autocast if PRECISION==\"autocast\" else nullcontext\n",
        "  ORIG_SEED = SEED\n",
        "  DRIVE_PIC_DIR = DRIVE_PIC_DIR.strip()\n",
        "\n",
        "  \n",
        "\n",
        "  # Enable third-party widgets\n",
        "  from google.colab import output\n",
        "  output.enable_custom_widget_manager()\n",
        "\n",
        "  \n",
        "  # Split this into a function\n",
        "  #  INPAINT_IMAGE = \"https://pbs.twimg.com/media/FbwYUfXaMAATXEj?format=jpg&name=large\" #@param {type:'string'}\n",
        "  # MASK_IMAGE = \"\" #@param {type:'string'}\n",
        "  # INPAINT_STRENGTH = 0.96 #@param {type:\"slider\", min:0, max:1, step:0.01} \n",
        "  if MODE == \"Inpainting\":\n",
        "    def download_image(url):\n",
        "      import requests\n",
        "      from io import BytesIO\n",
        "      response = requests.get(url)\n",
        "      return PIL.Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "    if 'http' in INPAINT_IMAGE:\n",
        "      INPAINT_IMAGE = download_image(INPAINT_IMAGE).resize((WIDTH, HEIGHT))\n",
        "    elif INPAINT_IMAGE:\n",
        "      INPAINT_IMAGE = PIL.Image.open(INPAINT_IMAGE)\n",
        "    if 'http' in MASK_IMAGE:\n",
        "      MASK_IMAGE = download_image(MASK_IMAGE).resize((WIDTH, HEIGHT))\n",
        "    elif MASK_IMAGE:\n",
        "      MASK_IMAGE = PIL.Image.open(MASK_IMAGE)\n",
        "    INPAINT_IMAGE.save(\"init.jpg\")\n",
        "    if not MASK_IMAGE:\n",
        "      import requests\n",
        "      from io import BytesIO\n",
        "      def draw(filename='drawing.png', color=\"white\", w=256, h=256, line_width=50,loop=False, init_img=\"init.jpg\"):\n",
        "        filename=\"init.jpg\"\n",
        "        import google\n",
        "        from IPython.display import HTML\n",
        "        from base64 import b64decode\n",
        "        import os\n",
        "        import shutil\n",
        "        import uuid\n",
        "        COLAB_HTML_ROOT = \"/usr/local/share/jupyter/nbextensions/google.colab/\"\n",
        "\n",
        "        def moveToExt(filename:str) -> str:\n",
        "          if not os.path.exists(filename):\n",
        "            print(\"Image file not found\")\n",
        "            return None\n",
        "          \n",
        "          target = os.path.basename(filename)\n",
        "          target = os.path.join(COLAB_HTML_ROOT, str(uuid.uuid4()) + target)\n",
        "          \n",
        "          shutil.copyfile(filename,target)\n",
        "          print(\"moved to ext\")\n",
        "          return target\n",
        "        real_filename = os.path.realpath(filename)\n",
        "        html_filename = real_filename\n",
        "        html_real_filename = html_filename\n",
        "        if os.path.exists(real_filename):\n",
        "          html_real_filename = moveToExt(real_filename)\n",
        "          html_filename = html_real_filename.replace(\"/usr/local/share/jupyter\",\"\")\n",
        "          \n",
        "\n",
        "        canvas_html = f\"\"\"\n",
        "      <canvas width={w} height={h}></canvas>\n",
        "\n",
        "      <div class=\"slidecontainer\">\n",
        "      <label for=\"lineWidth\" id=\"lineWidthLabel\">{line_width}px</label>\n",
        "        <input type=\"range\" min=\"1\" max=\"100\" value=\"1\" class=\"slider\" id=\"lineWidth\">\n",
        "      </div>\n",
        "\n",
        "      <div>\n",
        "        <button id=\"loadImage\">Reload from disk</button>\n",
        "        <button id=\"reset\">Reset</button>\n",
        "        <button id=\"save\">Save</button>\n",
        "      </div>\n",
        "      <script>\n",
        "\n",
        "      function loadImage(url) {{\n",
        "      return new Promise(r => {{ let i = new Image(); i.onload = (() => r(i)); i.src = url; }});\n",
        "      }}\n",
        "        \n",
        "        \n",
        "        var canvas = document.querySelector('canvas')\n",
        "        var ctx = canvas.getContext('2d')\n",
        "        ctx.lineWidth = {line_width};\n",
        "        \n",
        "        ctx.fillRect(0, 0, canvas.width, canvas.height);\n",
        "        ctx.strokeStyle = \"{color}\";\n",
        "\n",
        "\n",
        "        var slider = document.getElementById(\"lineWidth\");\n",
        "        slider.oninput = function() {{\n",
        "          ctx.lineWidth = this.value;\n",
        "          lineWidthLabel.innerHTML = `${{this.value}}px`\n",
        "        }}\n",
        "\n",
        "\n",
        "        function updateStroke(event){{\n",
        "            ctx.strokeStyle = event.target.value\n",
        "        }}\n",
        "        function updateBG(event){{\n",
        "            ctx.fillStyle = event.target.value\n",
        "        }}\n",
        "        \n",
        "        \n",
        "        var clear_button = document.querySelector('#reset')\n",
        "        var reload_img_button = document.querySelector('#loadImage')\n",
        "        \n",
        "        var button = document.querySelector('#save')\n",
        "\n",
        "        var mouse = {{x: 0, y: 0}}\n",
        "        canvas.addEventListener('mousemove', function(e) {{\n",
        "          mouse.x = e.pageX - this.offsetLeft\n",
        "          mouse.y = e.pageY - this.offsetTop\n",
        "        }})\n",
        "        canvas.onmousedown = ()=>{{\n",
        "          ctx.beginPath()\n",
        "          ctx.moveTo(mouse.x, mouse.y)\n",
        "          canvas.addEventListener('mousemove', onPaint)\n",
        "        }}\n",
        "        canvas.onmouseup = ()=>{{\n",
        "          canvas.removeEventListener('mousemove', onPaint)\n",
        "        }}\n",
        "        var onPaint = ()=>{{\n",
        "          ctx.lineTo(mouse.x, mouse.y)\n",
        "          ctx.stroke()\n",
        "        }}\n",
        "        reload_img_button.onclick = async ()=>{{\n",
        "          console.log(\"Reloading Image {html_filename}\")\n",
        "          let img = await loadImage('{html_filename}'); \n",
        "          console.log(\"Loaded image\")\n",
        "          ctx.drawImage(img, 0, 0);\n",
        "\n",
        "        }}\n",
        "        reload_img_button.click()\n",
        "      \n",
        "        clear_button.onclick = ()=>{{\n",
        "            console.log('Clearing Screen')\n",
        "            ctx.clearRect(0, 0, canvas.width, canvas.height);\n",
        "            ctx.fillRect(0, 0, canvas.width, canvas.height);\n",
        "          }}\n",
        "          canvas.addEventListener('load', function() {{\n",
        "          console.log('All assets are loaded')\n",
        "        }})\n",
        "        var data = new Promise(resolve=>{{\n",
        "          button.onclick = ()=>{{\n",
        "\n",
        "            var c = ctx\n",
        "            var imageData = ctx.getImageData(0,0, {w}, {h});\n",
        "            var pixel = imageData.data;\n",
        "            var r=0, g=1, b=2,a=3;\n",
        "          for (var p = 0; p<pixel.length; p+=4)\n",
        "          {{\n",
        "            if (\n",
        "                pixel[p+r] != 255 &&\n",
        "                pixel[p+g] != 255 &&\n",
        "                pixel[p+b] != 255) \n",
        "            {{pixel[p+r] =0; pixel[p+g]=0; pixel[p+b]=0}}\n",
        "          }}\n",
        "\n",
        "          c.putImageData(imageData,0,0);\n",
        "            resolve(canvas.toDataURL('image/png'))\n",
        "          }}\n",
        "          \n",
        "        }})\n",
        "        \n",
        "        \n",
        "      </script>\n",
        "      \"\"\"\n",
        "        print(HTML)\n",
        "        display(HTML(canvas_html))\n",
        "        print(\"Evaluating JS\")\n",
        "        \n",
        "        data = google.colab.output.eval_js(\"data\")\n",
        "        if data:\n",
        "          print(\"Saving Sketch\")  \n",
        "          binary = b64decode(data.split(',')[1])\n",
        "          # filename = html_real_filename if loop else filename\n",
        "          with open(\"init_mask.png\", 'wb') as f:\n",
        "            f.write(binary)\n",
        "          #return len(binary)\n",
        "\n",
        "\n",
        "\n",
        "      draw(filename = \"init_mask.png\", w=WIDTH, h=HEIGHT)\n",
        "      MASK_IMAGE = PIL.Image.open('init_mask.png')\n",
        "  if MODE == \"IMG2IMG\":\n",
        "    if 'http' in INIT_IMAGE:\n",
        "      import requests\n",
        "      from io import BytesIO\n",
        "      response = requests.get(INIT_IMAGE)\n",
        "      init_image = PIL.Image.open(BytesIO(response.content))\n",
        "      INIT_IMAGE = init_image\n",
        "    else:\n",
        "      if INIT_IMAGE == None or INIT_IMAGE == \"\":\n",
        "        if CLEAR_SETUP_LOG: from IPython.display import clear_output; clear_output()\n",
        "        print(\"No init image found, go ahead and draw your own below this text\")\n",
        "        def draw(filename='drawing.png', color=\"black\", bg_color=\"transparent\",w=256, h=256, line_width=1,loop=False):\n",
        "          import google\n",
        "          from IPython.display import HTML\n",
        "          from base64 import b64decode\n",
        "          import os\n",
        "          import shutil\n",
        "          import uuid\n",
        "          COLAB_HTML_ROOT = \"/usr/local/share/jupyter/nbextensions/google.colab/\"\n",
        "\n",
        "          def moveToExt(filename:str) -> str:\n",
        "            if not os.path.exists(filename):\n",
        "              print(\"Image file not found\")\n",
        "              return None\n",
        "            \n",
        "            target = os.path.basename(filename)\n",
        "            target = os.path.join(COLAB_HTML_ROOT, str(uuid.uuid4()) + target)\n",
        "            \n",
        "            shutil.copyfile(filename,target)\n",
        "            print(\"moved to ext\")\n",
        "            return target\n",
        "          real_filename = os.path.realpath(filename)\n",
        "          html_filename = real_filename\n",
        "          html_real_filename = html_filename\n",
        "          if os.path.exists(real_filename):\n",
        "            html_real_filename = moveToExt(real_filename)\n",
        "            html_filename = html_real_filename.replace(\"/usr/local/share/jupyter\",\"\")\n",
        "            \n",
        "\n",
        "          canvas_html = f\"\"\"\n",
        "        <canvas width={w} height={h}></canvas>\n",
        "        <div>\n",
        "          <label for=\"strokeColor\">Stroke</label>\n",
        "          <input type=\"color\" value=\"{color}\" id=\"strokeColor\">\n",
        "        \n",
        "          <label for=\"bgColor\">Background</label>\n",
        "          <input type=\"color\" value=\"{bg_color}\" id=\"bgColor\">\n",
        "        </div>\n",
        "        <div class=\"slidecontainer\">\n",
        "        <label for=\"lineWidth\" id=\"lineWidthLabel\">{line_width}px</label>\n",
        "          <input type=\"range\" min=\"1\" max=\"35\" value=\"1\" class=\"slider\" id=\"lineWidth\">\n",
        "        </div>\n",
        "\n",
        "        <div>\n",
        "          <button id=\"loadImage\">Reload from disk</button>\n",
        "          <button id=\"reset\">Reset</button>\n",
        "          <button id=\"save\">Save</button>\n",
        "          <button id=\"exit\">Exit</button>\n",
        "        </div>\n",
        "        <script>\n",
        "\n",
        "        function loadImage(url) {{\n",
        "        return new Promise(r => {{ let i = new Image(); i.onload = (() => r(i)); i.src = url; }});\n",
        "      }}\n",
        "          \n",
        "          \n",
        "          var canvas = document.querySelector('canvas')\n",
        "          var ctx = canvas.getContext('2d')\n",
        "          ctx.lineWidth = {line_width}\n",
        "          ctx.fillStyle = \"{bg_color}\";\n",
        "          \n",
        "          ctx.fillRect(0, 0, canvas.width, canvas.height);\n",
        "          ctx.strokeStyle = \"{color}\";\n",
        "\n",
        "          var strokeColor = document.querySelector('#strokeColor')\n",
        "          var bgColor = document.querySelector('#bgColor')\n",
        "\n",
        "          var slider = document.getElementById(\"lineWidth\");\n",
        "          slider.oninput = function() {{\n",
        "            ctx.lineWidth = this.value;\n",
        "            lineWidthLabel.innerHTML = `${{this.value}}px`\n",
        "          }}\n",
        "\n",
        "          function updateStroke(event){{\n",
        "              ctx.strokeStyle = event.target.value\n",
        "          }}\n",
        "          function updateBG(event){{\n",
        "              ctx.fillStyle = event.target.value\n",
        "          }}\n",
        "          \n",
        "          bgColor.addEventListener(\"change\", updateBG, false);\n",
        "          strokeColor.addEventListener(\"change\", updateStroke, false);\n",
        "          \n",
        "          var clear_button = document.querySelector('#reset')\n",
        "          var reload_img_button = document.querySelector('#loadImage')\n",
        "          var button = document.querySelector('#save')\n",
        "          var exit_button = document.querySelector('#exit')\n",
        "\n",
        "          var mouse = {{x: 0, y: 0}}\n",
        "          canvas.addEventListener('mousemove', function(e) {{\n",
        "            mouse.x = e.pageX - this.offsetLeft\n",
        "            mouse.y = e.pageY - this.offsetTop\n",
        "          }})\n",
        "          canvas.onmousedown = ()=>{{\n",
        "            ctx.beginPath()\n",
        "            ctx.moveTo(mouse.x, mouse.y)\n",
        "            canvas.addEventListener('mousemove', onPaint)\n",
        "          }}\n",
        "          canvas.onmouseup = ()=>{{\n",
        "            canvas.removeEventListener('mousemove', onPaint)\n",
        "          }}\n",
        "          var onPaint = ()=>{{\n",
        "            ctx.lineTo(mouse.x, mouse.y)\n",
        "            ctx.stroke()\n",
        "          }}\n",
        "          reload_img_button.onclick = async ()=>{{\n",
        "            console.log(\"Reloading Image {html_filename}\")\n",
        "            let img = await loadImage('{html_filename}'); \n",
        "            console.log(\"Loaded image\")\n",
        "            ctx.drawImage(img, 0, 0);\n",
        "\n",
        "          }}\n",
        "          \n",
        "          clear_button.onclick = ()=>{{\n",
        "              console.log('Clearing Screen')\n",
        "              ctx.clearRect(0, 0, canvas.width, canvas.height);\n",
        "              ctx.fillRect(0, 0, canvas.width, canvas.height);\n",
        "            }}\n",
        "            canvas.addEventListener('load', function() {{\n",
        "            console.log('All assets are loaded')\n",
        "          }})\n",
        "          var data = new Promise(resolve=>{{\n",
        "            button.onclick = ()=>{{\n",
        "              resolve(canvas.toDataURL('image/png'))\n",
        "            }}\n",
        "            exit_button.onclick = ()=>{{\n",
        "            resolve()\n",
        "          }}\n",
        "            \n",
        "          }})\n",
        "          \n",
        "          // window.onload = async ()=>{{\n",
        "          //   console.log(\"loaded\")\n",
        "          //   let img = await loadImage('{html_filename}');  \n",
        "          //   ctx.drawImage(img, 0, 0);\n",
        "          // }}\n",
        "          \n",
        "          \n",
        "        </script>\n",
        "        \"\"\"\n",
        "          print(HTML)\n",
        "          display(HTML(canvas_html))\n",
        "          print(\"Evaluating JS\")\n",
        "          \n",
        "          data = google.colab.output.eval_js(\"data\")\n",
        "          if data:\n",
        "            print(\"Saving Sketch\")  \n",
        "            binary = b64decode(data.split(',')[1])\n",
        "            # filename = html_real_filename if loop else filename\n",
        "            with open(filename, 'wb') as f:\n",
        "              f.write(binary)\n",
        "            #return len(binary)\n",
        "        \n",
        "\n",
        "\n",
        "        draw(filename = \"custom_image.png\", w=WIDTH, h=HEIGHT, bg_color=\"blue\", line_width=10)\n",
        "        INIT_IMAGE = \"/content/custom_image.png\"\n",
        "      INIT_IMAGE = PIL.Image.open(INIT_IMAGE)\n",
        "\n",
        "  # Check if upscalers are installed\n",
        "  if \"GFPGAN\" in IMAGE_UPSCALER:\n",
        "    GFPGAN_install()\n",
        "  if \"ESRGAN\"in IMAGE_UPSCALER:\n",
        "    ESRGAN_install()\n",
        "  if \"CodeFormer\" in IMAGE_UPSCALER:\n",
        "    CodeFormer_install()\n",
        "\n",
        "  try:\n",
        "    if MODE != LAST_MODE or LOW_VRAM_PATCH != LAST_VRAM or ENABLE_NSFW_FILTER != LAST_ENABLE_NSFW_FILTER or LAST_MODEL_ID != MODEL_ID or LAST_DIFFUSERS_VERSION != DIFFUSERS_VERSION:\n",
        "      print(\"Pipe specific settings have changed, repopulating pipe with new settings...\")\n",
        "      populate()\n",
        "  except NameError:\n",
        "    LAST_MODE = MODE\n",
        "    LAST_VRAM = LOW_VRAM_PATCH\n",
        "    LAST_ENABLE_NSFW_FILTER = ENABLE_NSFW_FILTER\n",
        "    LAST_MODEL_ID = MODEL_ID\n",
        "    LAST_DIFFUSERS_VERSION = DIFFUSERS_VERSION\n",
        "  if CLEAR_SETUP_LOG: from IPython.display import clear_output; clear_output()\n",
        "  # Make this into a function and create a function for anything repititive\n",
        "  PROMPTS = []\n",
        "  if PROMPT_FILE not in ['','none']:\n",
        "      try:\n",
        "          with open(PROMPT_FILE, \"r\") as f:\n",
        "              PROMPTS = f.read().splitlines()\n",
        "      except OSError as e:\n",
        "          raise e\n",
        "\n",
        "  if PROMPT not in ['', 'none']:\n",
        "      PROMPTS.insert(0, PROMPT)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    with precision_scope(\"cuda\"):\n",
        "        if RUN_FOREVER:\n",
        "          while True:\n",
        "            for pi in PROMPTS:\n",
        "              PROMPT = pi\n",
        "              print(OUTDIR)\n",
        "              if SAVE_PROMPT_DETAILS:\n",
        "                  epoch_time = int(time.time())\n",
        "                  try:\n",
        "                    with open(f'{OUTDIR}/{epoch_time}_prompt.txt', 'w') as file:\n",
        "                        file.write(f'{PROMPT}\\n\\nHeight: {HEIGHT}\\nWidth: {WIDTH}\\nSeed: {SEED}\\nScale: {SCALE}\\nPrecision: {PRECISION}\\nETA: {DDIM_ETA}\\nINIT_STRENGTH: {INIT_STRENGTH}\\nSCHEDULER: {SCHEDULER}')\n",
        "                  except FileNotFoundError:\n",
        "                    os.makedirs(OUTDIR)\n",
        "              for iteration in range(NUM_ITERS):\n",
        "                clean_env()\n",
        "                try:\n",
        "                  diffuse_run()\n",
        "                except KeyboardInterrupt:\n",
        "                  stop_its = True\n",
        "                  import os\n",
        "                  clean_env()\n",
        "                  try:\n",
        "                    image = None\n",
        "                  except Exception:\n",
        "                    pass\n",
        "                  raise SystemExit('\\33[33mExecution interrupted by user.\\33[0m')\n",
        "                except TypeError:\n",
        "                  make_pipe()\n",
        "                  make_scheduler()\n",
        "                  try:\n",
        "                    diffuse_run()\n",
        "                  except KeyboardInterrupt:\n",
        "                    stop_its = True\n",
        "                    import os\n",
        "                    clean_env()\n",
        "                    try:\n",
        "                      image = None\n",
        "                    except Exception:\n",
        "                      pass\n",
        "                    raise SystemExit('\\33[33mExecution interrupted by user.\\33[0m')\n",
        "                clean_env()\n",
        "        else:\n",
        "          for pi in PROMPTS:\n",
        "              PROMPT = pi\n",
        "              print(OUTDIR)\n",
        "              if SAVE_PROMPT_DETAILS:\n",
        "                  epoch_time = int(time.time())\n",
        "                  try:\n",
        "                    with open(f'{OUTDIR}/{epoch_time}_prompt.txt', 'w') as file:\n",
        "                        file.write(f'{PROMPT}\\n\\nHeight: {HEIGHT}\\nWidth: {WIDTH}\\nSeed: {SEED}\\nScale: {SCALE}\\nPrecision: {PRECISION}\\nETA:{DDIM_ETA}')\n",
        "                  except FileNotFoundError:\n",
        "                    os.makedirs(OUTDIR)\n",
        "              for iteration in range(NUM_ITERS):\n",
        "                clean_env()\n",
        "                try:\n",
        "                  diffuse_run()\n",
        "                except KeyboardInterrupt:\n",
        "                  stop_its = True\n",
        "                  import os\n",
        "                  clean_env()\n",
        "                  try:\n",
        "                    image = None\n",
        "                  except Exception:\n",
        "                    pass\n",
        "                  raise SystemExit('\\33[33mExecution interrupted by user.\\33[0m')\n",
        "                except TypeError:\n",
        "                  make_pipe()\n",
        "                  make_scheduler()\n",
        "                  try:\n",
        "                    diffuse_run()\n",
        "                  except KeyboardInterrupt:\n",
        "                    stop_its = True\n",
        "                    import os\n",
        "                    clean_env()\n",
        "                    try:\n",
        "                      image = None\n",
        "                    except Exception:\n",
        "                      pass\n",
        "                    raise SystemExit('\\33[33mExecution interrupted by user.\\33[0m')\n",
        "                clean_env()\n",
        "        clean_env()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <font color=\"orange\">**Clean Environment Up**</font>\n",
        "#@markdown <font size=\"3\">**Soft Reset** the environment by deleting pipes, models, and image handlers from memory. Use this when the VRAM gets stuck after changing pipes<br><br>\n",
        "#@markdown **Note:** Before using this cell, give a minute for the system itself to flush some stuff. This will give a higher chance of this function working.<br>\n",
        "#@markdown **Note 2:** Sometimes you'll get a persistent OOM bug when the GPU has been unallocated from your session. This is common with the new (09/2022) Free Colab Sessions</font>\n",
        "\n",
        "try:\n",
        "    del pipe; del transform; del prediction; del input_batch; del depth; del depth_image; del image; del sr_image; del enhanced_image; del img; del init; del original_init\n",
        "except NameError:\n",
        "    pass\n",
        "finally:\n",
        "    clean_env()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "FxvIfgi9YH4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-cd15ZYjLqf"
      },
      "source": [
        "## Diffuser Experiments (run through the Diffuser setup first)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jdSQSFlRjONj"
      },
      "outputs": [],
      "source": [
        "#@title Modifier Tester\n",
        "#@markdown `MODIFIER_FILE`: location of a text file with a list of modifiers seperated by newline.<br>You will need to upload it. Then right click on the file in colab and then click \"copy path\" (If you can't find it, click on the folder icon on the left pane). Then paste it in the box. See modifier_examples.txt for an example (if you're lazy, just edit the file. It will populate AFTER the first run)<br>\n",
        "#@markdown `BASE_PROMPT`: the prompt against which the modifiers will be tested<br>\n",
        "with open('modifier_examples.txt','w') as file:\n",
        "  file.write('Canon\\nNikon\\nPanasonic\\nSony\\nDigital Painting\\nMatte Painting\\nDrawing from a 5 year old\\nPasta Art\\nI made this while on acid')\n",
        "\n",
        "MODIFIER_FILE = \"/content/modifier_examples.txt\" #@param {type:'string'}\n",
        "BASE_PROMPT = \"A dog playing in a field\" #@param {type:'string'}\n",
        "STEPS = 50 #@param {type:\"slider\", min:5, max:500, step:5} \n",
        "SEED = 42 #@param {type:'integer'}\n",
        "WIDTH = 512 #@param {type:\"slider\", min:256, max:1920, step:64}\n",
        "HEIGHT = 512 #@param {type:\"slider\", min:256, max:1920, step:64}\n",
        "SCALE = 13.8 #@param {type:\"slider\", min:0, max:25, step:0.1}\n",
        "PRECISION = \"autocast\" #@param [\"full\",\"autocast\"]\n",
        "SEED = torch.Generator(\"cuda\").manual_seed(SEED)\n",
        "import random\n",
        "import torch\n",
        "from contextlib import contextmanager, nullcontext\n",
        "import time\n",
        "import os\n",
        "from torch import autocast\n",
        "\n",
        "OUTDIR = '/content/experiments'\n",
        "precision_scope = autocast if PRECISION==\"autocast\" else nullcontext\n",
        "with open(MODIFIER_FILE) as file:\n",
        "  for line in file.readlines():\n",
        "    line = line.strip()\n",
        "    PROMPT = f\"{BASE_PROMPT}, {line}\"\n",
        "    print(f\"Running: {PROMPT}\")\n",
        "    image = pipe(PROMPT, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, generator=SEED)[\"sample\"][0]  \n",
        "    display(image)\n",
        "    try:\n",
        "      image.save(f'{OUTDIR}/modifier_{line}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}.png')\n",
        "    except FileNotFoundError:\n",
        "      !mkdir $OUTDIR\n",
        "      image.save(f'{OUTDIR}/modifier_{line}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Q9upICRkMQcx"
      },
      "outputs": [],
      "source": [
        "#@title Randomizer, aka: I feel lucky/Fuck my shit up\n",
        "import os\n",
        "import random\n",
        "WORDS_AMOUNT = 30 #@param {type:\"integer\"}\n",
        "STEPS = 90 #@param {type:\"slider\", min:5, max:500, step:5}\n",
        "WIDTH = 512 #@param {type:\"slider\", min:256, max:1920, step:64}\n",
        "HEIGHT = 512 #@param {type:\"slider\", min:256, max:1920, step:64}\n",
        "SCALE = 13.7 #@param {type:\"slider\", min:0, max:25, step:0.1}\n",
        "PRECISION = \"autocast\" #@param [\"full\",\"autocast\"]\n",
        "precision_scope = autocast if PRECISION==\"autocast\" else nullcontext\n",
        "\n",
        "if not os.path.exists('words.txt'):\n",
        "  !wget https://gist.githubusercontent.com/h3xx/1976236/raw/bbabb412261386673eff521dddbe1dc815373b1d/wiki-100k.txt -O words.txt\n",
        "with open('words.txt') as file:\n",
        "  words = file.readlines()\n",
        "  prompt = \"\"\n",
        "  for iteration in range(WORDS_AMOUNT):\n",
        "    again = True\n",
        "    while again:\n",
        "      word = random.choice(words).strip()\n",
        "      if not '#' in word:\n",
        "        again = False\n",
        "    prompt += f'{random.choice(words).strip()}, '\n",
        "prompt = prompt[:-2]\n",
        "print(f'Prompt: {prompt}')\n",
        "with precision_scope(\"cuda\"):\n",
        "  seed = torch.Generator(\"cuda\").manual_seed(random.randint(0,4294967295))\n",
        "  image = pipe(prompt, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, generator=gen_seed)[\"sample\"][0] \n",
        "  display(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcnY9hUsy76f"
      },
      "source": [
        "#TXT2IMG Method (Needs fixing, diffusers right now is my priority)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BKZThsccD4Da"
      },
      "outputs": [],
      "source": [
        "#@title Huggingface Login\n",
        "from getpass import getpass\n",
        "\n",
        "huggingface_username = '' #@param {type:\"string\"}\n",
        "huggingface_token = '' #@param {type:\"string\"}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-HMZ9IiRDs8Q"
      },
      "outputs": [],
      "source": [
        "#@title TXT2IMG Setup\n",
        "import os\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "condacolab.install()\n",
        "root_code = root_model = \"/content/stableai\"\n",
        "code_dir = root_code + \"/stable-diffusion\"\n",
        "import os\n",
        "if not os.path.isdir(root_code):\n",
        "  !mkdir $root_code\n",
        "%cd $root_code\n",
        "!git clone https://github.com/DamascusGit/stable-diffusion.git\n",
        "!mamba env update -n base -f stable-diffusion/environment.yaml\n",
        "!pip install torchmetrics==0.6.0\n",
        "!pip install kornia==0.6)\n",
        "if not os.path.isdir(root_model):\n",
        "  !mkdir $root_model\n",
        "%cd $root_model\n",
        "!git lfs install\n",
        "!GIT_LFS_SKIP_SMUDGE=0\n",
        "# Will take a long time\n",
        "!git lfs clone https://$huggingface_username:$huggingface_token@huggingface.co/CompVis/stable-diffusion-v-1-4-original\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "def display_last_grid(grid_dir):\n",
        "  dir_list = os.listdir(grid_dir)\n",
        "  dir_list.sort()\n",
        "  #print (dir_list)\n",
        "  last_image = dir_list[-2]\n",
        "  img = Image.open(grid_dir + \"/\" + last_image).convert('RGB')\n",
        "  target_size = 600\n",
        "  img.thumbnail((target_size,target_size))\n",
        "  display (img)\n",
        "!mkdir /content/txt2img_output\n",
        "%cd $code_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9QnhfmAM0t-X"
      },
      "outputs": [],
      "source": [
        "\n",
        "import random\n",
        "import torch\n",
        "from contextlib import contextmanager, nullcontext\n",
        "import time\n",
        "import os\n",
        "from torch import autocast\n",
        "torch.cuda.empty_cache()\n",
        "PROMPT = \"matte potrait of a young cyberpunk woman as a Disney Princess, full-frame, complex picture, intricate, fine details, vogue, trending on artstation, artgerm, greg manchess, studio ghibli, Disney, Star Wars\" #@param {type:'string'}\n",
        "STEPS = 160 #@param {type:\"slider\", min:5, max:500, step:5} \n",
        "SEED = 0 #@param {type:'integer'}\n",
        "NUM_ITERS = 6 #@param {type:\"slider\", min:1, max:100, step:1} \n",
        "WIDTH = 512 #@param {type:\"slider\", min:256, max:1920, step:64}\n",
        "HEIGHT = 512 #@param {type:\"slider\", min:256, max:1920, step:64}\n",
        "SCALE = 13.8 #@param {type:\"slider\", min:0, max:25, step:0.1}\n",
        "PRECISION = \"autocast\" #@param [\"full\",\"autocast\"]\n",
        "SAVE_PROMPT_DETAILS = True #@param {type:\"boolean\"}\n",
        "USE_DRIVE_FOR_PICS = True #@param {type:\"boolean\"}\n",
        "DRIVE_PIC_DIR = \"AI_PICS\" #@param {type:\"string\"}\n",
        "IMAGE_UPSCALER = True #@param {type:\"boolean\"}\n",
        "UPSCALE_AMOUNT = \"2\" #@param [\"2\",\"4\", \"8\"]\n",
        "precision_scope = autocast if PRECISION==\"autocast\" else nullcontext\n",
        "ORIG_SEED = SEED\n",
        "\n",
        "%cd /content/\n",
        "\n",
        "if IMAGE_UPSCALER:\n",
        "  def upscale(image):\n",
        "    sr_image = model.predict(np.array(image))\n",
        "    return sr_image\n",
        "  if not os.path.exists('/content/Real-ESRGAN'):\n",
        "    !git clone https://github.com/sberbank-ai/Real-ESRGAN\n",
        "    !pip install -r Real-ESRGAN/requirements.txt\n",
        "    !gdown https://drive.google.com/uc?id=1pG2S3sYvSaO0V0B8QPOl1RapPHpUGOaV -O Real-ESRGAN/weights/RealESRGAN_x2.pth\n",
        "    !gdown https://drive.google.com/uc?id=1SGHdZAln4en65_NQeQY9UjchtkEF9f5F -O Real-ESRGAN/weights/RealESRGAN_x4.pth\n",
        "    !gdown https://drive.google.com/uc?id=1mT9ewx86PSrc43b-ax47l1E2UzR7Ln4j -O Real-ESRGAN/weights/RealESRGAN_x8.pth\n",
        "  %cd /content/Real-ESRGAN\n",
        "  from realesrgan import RealESRGAN\n",
        "  from PIL import Image\n",
        "  import numpy as np\n",
        "  import torch\n",
        "\n",
        "  device = torch.device('cuda')\n",
        "  model = RealESRGAN(device, scale = int(UPSCALE_AMOUNT))\n",
        "  model.load_weights(f'weights/RealESRGAN_x{UPSCALE_AMOUNT}.pth')\n",
        "  %cd /content/\n",
        "\n",
        "if USE_DRIVE_FOR_PICS and not os.path.exists('/content/drive'):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "if USE_DRIVE_FOR_PICS and not os.path.exists(f'/content/drive/MyDrive/{DRIVE_PIC_DIR}'):\n",
        "  !mkdir /content/drive/MyDrive/$DRIVE_PIC_DIR\n",
        "if USE_DRIVE_FOR_PICS:\n",
        "  OUTDIR = f'/content/drive/MyDrive/{DRIVE_PIC_DIR}'\n",
        "else:\n",
        "  OUTDIR = '/content/diffusers_output'\n",
        "epoch_time = int(time.time())\n",
        "if SAVE_PROMPT_DETAILS:\n",
        "  with open(f'{OUTDIR}/{epoch_time}_prompt.txt', 'w') as file:\n",
        "        file.write(f'{PROMPT}\\n\\nHeight: {HEIGHT}\\nWidth: {WIDTH}\\nSeed: {SEED}\\nScale: {SCALE}\\nPrecision: {PRECISION}\\n')\n",
        "with precision_scope(\"cuda\"):\n",
        "  for iteration in range(NUM_ITERS):\n",
        "    \n",
        "    if ORIG_SEED == 0:\n",
        "      rand_num = random.randint(0,4294967295)\n",
        "      gen_seed = torch.Generator(\"cuda\").manual_seed(rand_num)\n",
        "    else:\n",
        "      gen_seed = torch.Generator(\"cuda\").manual_seed(SEED)\n",
        "    epoch_time = int(time.time())\n",
        "    try:\n",
        "      print(f'Seed: {rand_num}, Scale: {SCALE}, Steps: {STEPS}')\n",
        "    except NameError:\n",
        "      print(f'Seed: {SEED}, Scale: {SCALE}, Steps: {STEPS}')\n",
        "    \n",
        "    image = pipe(PROMPT, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, generator=gen_seed)[\"sample\"][0]  \n",
        "    display(image)\n",
        "    try:\n",
        "      image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{rand_num}.png')\n",
        "    except NameError:\n",
        "      image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}.png')\n",
        "    print('Upscaling... ')\n",
        "    sr_image = upscale(image)\n",
        "    display(sr_image)\n",
        "    try:\n",
        "      sr_image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{rand_num}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "    except NameError:\n",
        "      sr_image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "#@markdown If you're using the `low VRAM patch` you <b>HAVE</b> to use `autocast`<br>\n",
        "#@markdown `Out of Memory error`: If the VRAM stacks right after execution, sometimes it helps waiting for a minute before running it again. Looking at ways to force it to clear the VRAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQeX4o971T_U"
      },
      "source": [
        "#How to install offline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFdp-FFg1cao"
      },
      "source": [
        "https://rentry.org/SDInstallation"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "TeIWggi6TGH0",
        "OPyJJ2z-RJB7",
        "Z-cd15ZYjLqf",
        "WcnY9hUsy76f",
        "HQeX4o971T_U"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}